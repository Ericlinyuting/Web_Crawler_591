{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "newhouse591.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ericlinyuting/Web_Crawler_591/blob/main/newhouse591_without_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQNEwUtkphBS",
        "outputId": "5c85aff3-4cae-4fbe-e195-2c284db15b6a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQJAPHxbomd8"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dh0bQF-HEgw"
      },
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import csv\n",
        "import io\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERMnx-cnoeIn"
      },
      "source": [
        "headers={\n",
        "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "    }\n",
        "\n",
        "request_url='http://newhouse.591.com.tw/home/housing/info?hid=119282'\n",
        "res=requests.get(request_url, headers = headers)\n",
        "bs=BeautifulSoup(res.text,'html.parser')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDXNjxovoeIo"
      },
      "source": [
        "# input 建案網址 return 建案建案詳情欄位\n",
        "# 產出為建案名與16個建案資訊\n",
        "def getData(url):\n",
        "    request_url='https://newhouse.591.com.tw/home/housing/info?hid='+str(url).strip()\n",
        "    headers={\n",
        "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "    }\n",
        "    res=requests.get(request_url, headers = headers)\n",
        "    #bs=BeautifulSoup(res.text,'html.parser')\n",
        "\n",
        "    request_url_detail='https://newhouse.591.com.tw/home/housing/info?hid='+str(url).strip()+\"/detail\"\n",
        "    res_detail=requests.get(request_url_detail, headers = headers)\n",
        "\n",
        "    if res.status_code == 200 & res_detail.status_code==200:\n",
        "        bs=BeautifulSoup(res.text,'html.parser')\n",
        "        bs_detail=BeautifulSoup(res_detail.text,'html.parser')\n",
        "        #先宣告變數為NULL 若無撈到資料則寫入NULL\n",
        "        title=\"NULL\"\n",
        "\n",
        "        # 利用 beautfiulsoup 的 find function 利用 css selector 定位 並撈出指定資料\n",
        "        # 利用 try except 若無撈到資料則寫入NULL\n",
        "        title=bs.find('h1').text\n",
        "        try:\n",
        "          tag = ', '.join([span.text.strip() for span in bs.find(\"p\", {'class':\"build-tag\"}).find_all(\"span\")]) if bs.find(\"p\", {'class':\"build-tag\"}) else \"未找到相應標籤\" #建案標籤\n",
        "        except:\n",
        "          tag = \"NULL\"\n",
        "        try:\n",
        "          unit_price=bs.find(\"span\", {'class':\"price\"}).text #單價\n",
        "        except:\n",
        "          unit_price= \"NULL\"\n",
        "        try:\n",
        "          unit = bs.find(\"span\", {'class':\"unit\"}).text #單位\n",
        "        except:\n",
        "          unit = \"NULL\"\n",
        "        try:\n",
        "          material = bs.find(\"h4\",text=\"建材說明\").findNext(\"p\").text #建材說明\n",
        "        except:\n",
        "          material= \"NULL\"\n",
        "        try:\n",
        "          htype=bs_detail.find(\"span\", text=\"建案類別\").findNext(\"p\").string.strip() #建案類別\n",
        "        except:\n",
        "          htype= \"NULL\"\n",
        "        try:\n",
        "          htype2=bs_detail.find(\"span\", text=\"建案型態\").findNext(\"p\").string.strip().replace(' ', '').replace('\\n', '、') #建案型態\n",
        "        except:\n",
        "          htype2= \"NULL\"\n",
        "        try:\n",
        "          htype3=bs_detail.find(\"span\", text=\"公開銷售\").findNext(\"p\").string.strip() #公開銷售\n",
        "        except:\n",
        "          htype3= \"NULL\"\n",
        "        try:\n",
        "          htype4=bs_detail.find(\"p\", {'class':\"address\"}).findNext(\"span\").text #基地地址\n",
        "        except:\n",
        "          htype4= \"NULL\"\n",
        "        try:\n",
        "          htype5=bs_detail.find(\"span\", text=\"交屋屋況\").findNext(\"p\").string.strip() #交屋屋況\n",
        "        except:\n",
        "          htype5= \"NULL\"\n",
        "        try:\n",
        "          htype6=bs_detail.find(\"span\",text=\"格局規劃\").findNext(\"p\").text.strip().replace(' ', '') #格局規劃\n",
        "        except:\n",
        "          htype6= \"NULL\"\n",
        "        try:\n",
        "          htype7=bs_detail.find(\"span\", text=\"投資建設\").findNext(\"p\").string.strip() #投資建設\n",
        "        except:\n",
        "          htype7= \"NULL\"\n",
        "        try:\n",
        "          htype8=bs_detail.find(\"span\", text=\"營造公司\").findNext(\"p\").string.strip() #營造公司\n",
        "        except:\n",
        "          htype8= \"NULL\"\n",
        "        try:\n",
        "          htype9=bs_detail.find(\"span\", text=\"棟戶規劃\").findNext(\"p\").string.strip() #棟戶規劃\n",
        "        except:\n",
        "          htype9= \"NULL\"\n",
        "        try:\n",
        "          htype10=bs_detail.find(\"span\", text=\"樓層規劃\").findNext(\"p\").string.strip() #樓層規劃\n",
        "        except:\n",
        "          htype10= \"NULL\"\n",
        "        try:\n",
        "          htype11=bs_detail.find(\"span\", text=\"用途規劃\").findNext(\"p\").string.strip() #用途規劃\n",
        "        except:\n",
        "          htype11= \"NULL\"\n",
        "        return title,tag,unit_price,unit,material,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11\n",
        "    else:\n",
        "        print('link expired:', url)\n",
        "        return 404, 404, 404, 404, 404, 404, 404"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNJrdA_kn6a2"
      },
      "source": [
        "#def get_dynamic_data(url):\n",
        "#    request_url='https://newhouse.591.com.tw/home/housing/dynamic?hid='+str(url).strip()\n",
        "#    headers={\n",
        "#    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "#    }\n",
        "#    res=requests.get(request_url, headers = headers)\n",
        "#    #bs=BeautifulSoup(res.text,'html.parser')\n",
        "#\n",
        "#    if res.status_code == 200:\n",
        "#        bs=BeautifulSoup(res.text,'html.parser')\n",
        "#        #先宣告變數為NULL 若無撈到資料則寫入NULL\n",
        "#        news_date = 'NULL'\n",
        "#        news = 'NULL'\n",
        "#        # 利用 beautfiulsoup 的 find function 利用 css selector 定位 並撈出指定資料\n",
        "#        news_date = bs.find(\"div\", {'class':\"dynamic-date\"}).findNext(\"span\").contents[0].string.strip() + \"-\" +\\\n",
        "#                    bs.find(\"div\", {'class':\"dynamic-date\"}).findNext(\"strong\").contents[0].string.strip()\n",
        "#\n",
        "#        news = bs.find(\"div\", {'class':\"dynamic-infolist\"}).string\n",
        "#\n",
        "#        return news_date, news\n",
        "#    else:\n",
        "#        print('link expired:', url)\n",
        "#        return 404, 404, 404, 404, 404, 404, 404"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiM1Bqd_qNTz"
      },
      "source": [
        "def main(outputfile, rid, sid, totalpages):\n",
        "\n",
        "   with io.open(outputfile, \"w\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([ \"建案名稱\", \"建案標籤\",\"單價\",\"單位\",\"建材說明\" ,\"建案類別\", \"建物形態\", \"公開銷售\", \"基地地址\", \"交屋屋況\"\\\n",
        "                        ,\"格局規劃\", \"投資建設\", \"營造公司\", \"棟戶規劃\", \"樓層規劃\", \"用途規劃\", \"網址\"]) \\\n",
        "                        #,\"動態資訊日期日期\", \"動態資訊\"])\n",
        "        totalpages = totalpages\n",
        "        print('Total pages: ', totalpages)\n",
        "\n",
        "        for i in range(1, totalpages+1):\n",
        "            request_url = \"https://newhouse.591.com.tw/home/housing/search?rid=\"+str(rid)+\"&sid=\"+str(sid)+\"&page=\"+str(i)\n",
        "            response = requests.get(request_url, headers = headers)\n",
        "            response = response.json()\n",
        "            items = response[\"data\"][\"items\"]\n",
        "\n",
        "            house_url_list=[] #存放網址list\n",
        "            for key in items:\n",
        "                id = key[\"hid\"] # 每個物件的 id\n",
        "                house_url_list.append(id)\n",
        "            #if totalpages%14 == 0:\n",
        "            #  time.sleep(180)\n",
        "            #else:\n",
        "            time.sleep(random.randint(5,100)) #反爬蟲\n",
        "\n",
        "            # ------------- write into csv ------------- #\n",
        "            for url in house_url_list:\n",
        "                title,tag,unit_price,unit,material,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11 = getData(url)\n",
        "                #news_date, news = get_dynamic_data(url)\n",
        "                writer.writerow([title,tag,unit_price,unit,material,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11,\\\n",
        "                                 str('https://newhouse.591.com.tw/home/housing/info?hid='+str(url))])\n",
        "                                 #, news_date, news])\n",
        "                #print(\"url=\",str('https://newhouse.591.com.tw/home/housing/info?hid='+str(url)))\n",
        "            #print(\"rid=\",rid,\",sid=\",sid,\",page=\",i,)\n",
        "            # ------------------------------------------ #\n",
        "            print(i/totalpages*100, '%') # print out 完成 %數"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XXxgt867oeIp",
        "outputId": "e1808faa-8973-4177-98cc-f94cda8746fb"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # -------- configurable parameter -------- #\n",
        "    # 以台北市不限區舉例(預設網址可能沒寫rid&sid, 點選縣市或區往只會顯示如下)\n",
        "    # link:https://newhouse.591.com.tw/housing-list.html?rid=1&sid=0\n",
        "\n",
        "    request_url = \"https://newhouse.591.com.tw/home/housing/search?rid=0&sid=0\"\n",
        "    headers={\n",
        "        'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "        }\n",
        "    response = requests.get(request_url, headers = headers)\n",
        "    response = response.json()\n",
        "    total = response[\"data\"][\"total\"]\n",
        "    per_page = response[\"data\"][\"per_page\"]\n",
        "\n",
        "    output_file_name = '/content/drive/MyDrive/output.csv' #設定存放位置與檔名\n",
        "    rid = 0           # 設定縣市 (台北市 rid = 1)\n",
        "    sid = 0           # 設定地區 (不限區 sid = 0)\n",
        "    totalpages = math.ceil(total/per_page)    # 設定抓取頁數\n",
        "    # ---------------------------------------- #\n",
        "\n",
        "    main(output_file_name, rid, sid, totalpages)                                          #匯出csv檔\n",
        "    read_file = pd.read_csv (output_file_name)\n",
        "    read_file.to_excel ('/content/drive/MyDrive/output.xlsx', index = None, header=True) #匯出excel檔\n",
        "    print('\\nfinish!')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages:  782\n",
            "rid= 0 ,sid= 0 ,page= 1\n",
            "0.1278772378516624 %\n",
            "rid= 0 ,sid= 0 ,page= 2\n",
            "0.2557544757033248 %\n",
            "rid= 0 ,sid= 0 ,page= 3\n",
            "0.3836317135549872 %\n",
            "rid= 0 ,sid= 0 ,page= 4\n",
            "0.5115089514066496 %\n",
            "rid= 0 ,sid= 0 ,page= 5\n",
            "0.639386189258312 %\n",
            "rid= 0 ,sid= 0 ,page= 6\n",
            "0.7672634271099744 %\n",
            "rid= 0 ,sid= 0 ,page= 7\n",
            "0.8951406649616368 %\n",
            "rid= 0 ,sid= 0 ,page= 8\n",
            "1.0230179028132993 %\n",
            "rid= 0 ,sid= 0 ,page= 9\n",
            "1.1508951406649617 %\n",
            "rid= 0 ,sid= 0 ,page= 10\n",
            "1.278772378516624 %\n",
            "rid= 0 ,sid= 0 ,page= 11\n",
            "1.4066496163682864 %\n",
            "rid= 0 ,sid= 0 ,page= 12\n",
            "1.5345268542199488 %\n",
            "rid= 0 ,sid= 0 ,page= 13\n",
            "1.6624040920716114 %\n",
            "rid= 0 ,sid= 0 ,page= 14\n",
            "1.7902813299232736 %\n",
            "rid= 0 ,sid= 0 ,page= 15\n",
            "1.9181585677749362 %\n",
            "rid= 0 ,sid= 0 ,page= 16\n",
            "2.0460358056265986 %\n",
            "rid= 0 ,sid= 0 ,page= 17\n",
            "2.1739130434782608 %\n",
            "rid= 0 ,sid= 0 ,page= 18\n",
            "2.3017902813299234 %\n",
            "rid= 0 ,sid= 0 ,page= 19\n",
            "2.4296675191815855 %\n",
            "rid= 0 ,sid= 0 ,page= 20\n",
            "2.557544757033248 %\n",
            "rid= 0 ,sid= 0 ,page= 21\n",
            "2.6854219948849107 %\n",
            "rid= 0 ,sid= 0 ,page= 22\n",
            "2.813299232736573 %\n",
            "rid= 0 ,sid= 0 ,page= 23\n",
            "2.941176470588235 %\n",
            "rid= 0 ,sid= 0 ,page= 24\n",
            "3.0690537084398977 %\n",
            "rid= 0 ,sid= 0 ,page= 25\n",
            "3.1969309462915603 %\n",
            "rid= 0 ,sid= 0 ,page= 26\n",
            "3.324808184143223 %\n",
            "rid= 0 ,sid= 0 ,page= 27\n",
            "3.4526854219948846 %\n",
            "rid= 0 ,sid= 0 ,page= 28\n",
            "3.580562659846547 %\n",
            "rid= 0 ,sid= 0 ,page= 29\n",
            "3.70843989769821 %\n",
            "rid= 0 ,sid= 0 ,page= 30\n",
            "3.8363171355498724 %\n",
            "rid= 0 ,sid= 0 ,page= 31\n",
            "3.9641943734015346 %\n",
            "link expired: 133689\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-636a1f0e25e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# ---------------------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalpages\u001b[0m\u001b[0;34m)\u001b[0m                                          \u001b[0;31m#匯出csv檔\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mread_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mread_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/output.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#匯出excel檔\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a1b7d9515e1e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(outputfile, rid, sid, totalpages)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# ------------- write into csv ------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhouse_url_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit_price\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaterial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;31m#news_date, news = get_dynamic_data(url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 writer.writerow([title,tag,unit_price,unit,material,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11,\\\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 16, got 7)"
          ]
        }
      ]
    }
  ]
}