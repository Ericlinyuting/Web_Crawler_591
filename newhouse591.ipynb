{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "newhouse591.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucaChuang/Newhouse-591-Web-Scraping/blob/main/newhouse591.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQNEwUtkphBS",
        "outputId": "00f81774-57f5-42b8-9009-8505a888c1d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQJAPHxbomd8",
        "outputId": "96eee836-8223-41b3-bb90-0fe784069eb4"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [577 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,964 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,396 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,745 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,394 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,163 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [893 kB]\n",
            "Fetched 11.4 MB in 4s (2,864 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 81.0 MB of archives.\n",
            "After this operation, 273 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\n",
            "Fetched 81.0 MB in 5s (15.3 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dh0bQF-HEgw"
      },
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import csv\n",
        "import io\n",
        "import json\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERMnx-cnoeIn"
      },
      "source": [
        "headers={\n",
        "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "    }\n",
        "\n",
        "request_url='http://newhouse.591.com.tw/home/housing/info?hid=119282'\n",
        "res=requests.get(request_url, headers = headers)\n",
        "bs=BeautifulSoup(res.text,'html.parser')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDXNjxovoeIo"
      },
      "source": [
        "# input 建案網址 return 建案建案詳情欄位\n",
        "# 產出為建案名與12個建案資訊\n",
        "def getData(url):\n",
        "    request_url='https://newhouse.591.com.tw/home/housing/info?hid='+str(url).strip()\n",
        "    headers={\n",
        "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "    }\n",
        "    res=requests.get(request_url, headers = headers)\n",
        "    #bs=BeautifulSoup(res.text,'html.parser')\n",
        "\n",
        "    if res.status_code == 200:\n",
        "        bs=BeautifulSoup(res.text,'html.parser')\n",
        "        #先宣告變數為NULL 若無撈到資料則寫入NULL\n",
        "        title='NULL'\n",
        "        cost='NULL'\n",
        "        htype='NULL'\n",
        "        htype2='NULL'\n",
        "        htype3='NULL'\n",
        "        htype4='NULL'\n",
        "        htype5='NULL'\n",
        "        htype6='NULL'\n",
        "        htype7='NULL'\n",
        "        htype8='NULL'\n",
        "        htype9='NULL'\n",
        "        htype10='NULL'\n",
        "        htype11='NULL'\n",
        "\n",
        "        # 利用 beautfiulsoup 的 find function 利用 css selector 定位 並撈出指定資料\n",
        "        title=bs.find('h1').text\n",
        "        cost=bs.find(\"dl\", {'class':\"clearfix\"}).findNext(\"dd\").text.strip()\n",
        "        htype=bs.find(\"dt\", text=\"建案類別：\").findNext(\"dd\").string.strip()\n",
        "        htype2=bs.find(\"dt\", text=\"建物形態：\").findNext(\"dd\").string.strip().replace(' ', '').replace('\\n', '、')\n",
        "        htype3=bs.find(\"dt\", text=\"公開銷售：\").findNext(\"dd\").string.strip()\n",
        "        htype4=bs.find(\"dt\", text=\"基地地址：\").findNext(\"dd\").contents[0].string.strip()\n",
        "        htype5=bs.find(\"dt\", text=\"交屋屋況：\").findNext(\"dd\").string.strip()\n",
        "        htype6=bs.find(\"div\",{'class':\"flex_5 stonefont\"}).findNext('span',{'class':''}).text.strip().replace(' ', '')\n",
        "        htype7=bs.find(\"dt\", text=\"投資建設：\").findNext(\"dd\").string.strip()\n",
        "        htype8=bs.find(\"dt\", text=\"營造公司：\").findNext(\"dd\").string.strip()\n",
        "        htype9=bs.find(\"dt\", text=\"棟戶規劃：\").findNext(\"dd\").string.strip()\n",
        "        htype10=bs.find(\"dt\", text=\"樓層規劃：\").findNext(\"dd\").string.strip()\n",
        "        htype11=bs.find(\"dt\", text=\"用途規劃：\").findNext(\"dd\").string.strip()\n",
        "                \n",
        "        return title,cost,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11\n",
        "    else:\n",
        "        print('link expired:', url)\n",
        "        return 404, 404, 404, 404, 404, 404, 404"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNJrdA_kn6a2"
      },
      "source": [
        "def get_dynamic_data(url):\n",
        "    request_url='https://newhouse.591.com.tw/home/housing/dynamic?hid='+str(url).strip()\n",
        "    headers={\n",
        "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "    }\n",
        "    res=requests.get(request_url, headers = headers)\n",
        "    #bs=BeautifulSoup(res.text,'html.parser')\n",
        "\n",
        "    if res.status_code == 200:\n",
        "        bs=BeautifulSoup(res.text,'html.parser')\n",
        "        #先宣告變數為NULL 若無撈到資料則寫入NULL\n",
        "        news_date = 'NULL'\n",
        "        news = 'NULL'\n",
        "        # 利用 beautfiulsoup 的 find function 利用 css selector 定位 並撈出指定資料\n",
        "        news_date = bs.find(\"div\", {'class':\"dynamic-date\"}).findNext(\"span\").contents[0].string.strip() + \"-\" +\\\n",
        "                    bs.find(\"div\", {'class':\"dynamic-date\"}).findNext(\"strong\").contents[0].string.strip()\n",
        "\n",
        "        news = bs.find(\"div\", {'class':\"dynamic-infolist\"}).string\n",
        "                \n",
        "        return news_date, news\n",
        "    else:\n",
        "        print('link expired:', url)\n",
        "        return 404, 404, 404, 404, 404, 404, 404"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiM1Bqd_qNTz"
      },
      "source": [
        "def main(outputfile, rid, sid, totalpages):\n",
        "   \n",
        "   with io.open(outputfile, \"w\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"建案名稱\", \"單價\", \"建案類別\", \"建物形態\", \"公開銷售\", \"基地地址\", \"交屋屋況\"\\\n",
        "                        ,\"格局規劃\", \"投資建設\", \"營造公司\", \"棟戶規劃\", \"樓層規劃\", \"用途規劃\", \"網址\"\\\n",
        "                        ,\"動態資訊日期日期\", \"動態資訊\"])\n",
        "        totalpages = totalpages\n",
        "        print('Total pages: ', totalpages)\n",
        "\n",
        "        for i in range(1, totalpages+1):\n",
        "            request_url = \"https://newhouse.591.com.tw/home/housing/search?rid=\"+str(rid)+\"&sid=\"+str(sid)+\"&page=\"+str(i)\n",
        "            response = requests.get(request_url, headers = headers)\n",
        "            response = response.json()\n",
        "            items = response[\"data\"][\"items\"]\n",
        "\n",
        "            house_url_list=[] #存放網址list\n",
        "            for key in items:\n",
        "                id = key[\"hid\"] # 每個物件的 id\n",
        "                house_url_list.append(id)\n",
        "            time.sleep(3)\n",
        "            #time.sleep(random.randint(5,10))\n",
        "\n",
        "            # ------------- write into csv ------------- #\n",
        "            for url in house_url_list:\n",
        "                title,cost,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11 = getData(url)\n",
        "                try:\n",
        "                  news_date, news = get_dynamic_data(url)\n",
        "                except:\n",
        "                  news_date = \"Null\"\n",
        "                  news = \"Null\"\n",
        "                  print(\"No news\")\n",
        "                writer.writerow([title,cost,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11,\\\n",
        "                                 str('https://newhouse.591.com.tw/home/housing/info?hid='+str(url)), news_date, news])\n",
        "            # ------------------------------------------ #\n",
        "            print(i/totalpages*100, '%') # print out 完成 %數"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXxgt867oeIp",
        "outputId": "ad7be907-dce7-4304-b493-bbaad1478216"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # -------- configurable parameter -------- #\n",
        "    # 以台北市不限區舉例(預設網址可能沒寫rid&sid, 點選縣市或區往只會顯示如下)\n",
        "    # link:https://newhouse.591.com.tw/housing-list.html?rid=1&sid=0\n",
        "\n",
        "    output_file_name = '/content/drive/MyDrive/output.csv' #設定存放位置與檔名\n",
        "    rid = 1           # 設定縣市 (台北市 rid = 1)\n",
        "    sid = 0           # 設定地區 (不限區 sid = 0)\n",
        "    totalpages = 16    # 設定抓取頁數 # 14頁大概\n",
        "    # ---------------------------------------- #\n",
        "\n",
        "    main(output_file_name, rid, sid, totalpages)                                          #匯出csv檔\n",
        "    read_file = pd.read_csv (output_file_name)\n",
        "    read_file.to_excel ('/content/drive/MyDrive/output.xlsx', index = None, header=True) #匯出excel檔\n",
        "    print('\\nfinish!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total pages:  16\n",
            "No news\n",
            "93.75 %\n",
            "100.0 %\n",
            "\n",
            "finish!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}