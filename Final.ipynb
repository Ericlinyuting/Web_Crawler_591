{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Joseph project 0204.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dh0bQF-HEgw"
      },
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import csv\n",
        "import io\n",
        "import json\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "headers={\n",
        "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "    }\n",
        "\n",
        "request_url='http://newhouse.591.com.tw/home/housing/info?hid=119282'\n",
        "res=requests.get(request_url, headers = headers)\n",
        "bs=BeautifulSoup(res.text,'html.parser')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input 物件網址 撈取網頁資料 return 欄位 \n",
        "def getData(url):\n",
        "    request_url='https://newhouse.591.com.tw/home/housing/info?hid='+str(url).strip()\n",
        "    headers={\n",
        "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'\n",
        "    }\n",
        "    res=requests.get(request_url, headers = headers)\n",
        "    #bs=BeautifulSoup(res.text,'html.parser')\n",
        "\n",
        "    if res.status_code == 200:\n",
        "        bs=BeautifulSoup(res.text,'html.parser')\n",
        "        #先宣告變數為NULL 若無撈到資料則寫入NULL\n",
        "        title='NULL'\n",
        "        addr='NULL'\n",
        "        htype='NULL'\n",
        "        htype2='NULL'\n",
        "        htype3='NULL'\n",
        "        htype4='NULL'\n",
        "        htype5='NULL'\n",
        "        htype6='NULL'\n",
        "        htype7='NULL'\n",
        "        htype8='NULL'\n",
        "        htype9='NULL'\n",
        "        htype10='NULL'\n",
        "        htype11='NULL'\n",
        "\n",
        "        # 利用 beautfiulsoup 的 find function 利用 css selector 定位 並撈出指定資料\n",
        "        title=bs.find('h1').text\n",
        "        addr=bs.find(\"dl\", {'class':\"clearfix\"}).findNext(\"dd\").text.strip()\n",
        "        htype=bs.find(\"dt\", text=\"建案類別：\").findNext(\"dd\").string.strip()\n",
        "        htype2=bs.find(\"dt\", text=\"建物形態：\").findNext(\"dd\").string.strip().replace(' ', '').replace('\\n', '、')\n",
        "        htype3=bs.find(\"dt\", text=\"公開銷售：\").findNext(\"dd\").string.strip()\n",
        "        htype4=bs.find(\"dt\", text=\"基地地址：\").findNext(\"dd\").contents[0].string.strip()\n",
        "        htype5=bs.find(\"dt\", text=\"交屋屋況：\").findNext(\"dd\").string.strip()\n",
        "        htype6=bs.find(\"div\",{'class':\"flex_5 stonefont\"}).findNext('span',{'class':''}).text.strip().replace(' ', '')\n",
        "        htype7=bs.find(\"dt\", text=\"投資建設：\").findNext(\"dd\").string.strip()\n",
        "        htype8=bs.find(\"dt\", text=\"營造公司：\").findNext(\"dd\").string.strip()\n",
        "        htype9=bs.find(\"dt\", text=\"棟戶規劃：\").findNext(\"dd\").string.strip()\n",
        "        htype10=bs.find(\"dt\", text=\"樓層規劃：\").findNext(\"dd\").string.strip()\n",
        "        htype11=bs.find(\"dt\", text=\"用途規劃：\").findNext(\"dd\").string.strip()\n",
        "                \n",
        "        return title,addr,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11\n",
        "    else:\n",
        "        print('link expired:', url)\n",
        "        return 404, 404, 404, 404, 404, 404, 404\n",
        "\n",
        "\n",
        "\n",
        "def main(outputfile, rid, sid):\n",
        "   \n",
        "   with io.open(outputfile, \"w\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        \n",
        "        totalpages = 5\n",
        "        print('Total pages: ', totalpages)\n",
        "\n",
        "        for i in range(1, totalpages+1):\n",
        "            request_url = \"https://newhouse.591.com.tw/home/housing/search?rid=\"+str(rid)+\"&sid=\"+str(sid)+\"&page=\"+str(i)\n",
        "            response = requests.get(request_url, headers = headers)\n",
        "            response = response.json()\n",
        "            items = response[\"data\"][\"items\"]\n",
        "\n",
        "            room_url_list=[] #存放網址list\n",
        "            for key in items:\n",
        "                id = key[\"hid\"] # 每個物件的 id\n",
        "                room_url_list.append(id)\n",
        "            time.sleep(3)\n",
        "\n",
        "            # ------------- write into csv ------------- #\n",
        "#            info_url_list=[]\n",
        "#            for url in list(set(room_url_list)):\n",
        "#                url = url.replace(\"detail?\", \"info?\")\n",
        "#                info_url_list.append(url)\n",
        "            for url in room_url_list:\n",
        "                title,addr,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11 = getData(url)\n",
        "                writer.writerow([title,addr,htype, htype2, htype3, htype4, htype5, htype6, htype7, htype8, htype9, htype10, htype11])\n",
        "            # ------------------------------------------ #\n",
        "            print(i/totalpages*100, '%') # print out 完成 %數"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages:  5\n",
            "20.0 %\n",
            "40.0 %\n",
            "60.0 %\n",
            "80.0 %\n",
            "100.0 %\n",
            "\n",
            "finish!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # -------- configurable parameter -------- #\n",
        "    output_file_name = 'tpe_rent_output.csv'\n",
        "    # ---------------------------------------- #\n",
        "    main(output_file_name, 1, 1)\n",
        "    read_file = pd.read_csv (output_file_name)\n",
        "    read_file.to_excel ('tpe_rent_output.xlsx', index = None, header=False)\n",
        "    print('\\nfinish!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}